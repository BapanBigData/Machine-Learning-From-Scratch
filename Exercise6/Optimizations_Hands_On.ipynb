{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "x = cp.Variable()\n",
    "y = cp.Variable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(cp.Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable((), var1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable((), var2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Minimize(Expression(CONVEX, NONNEGATIVE, ()))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the objective function\n",
    "objective = cp.Minimize(x**2 + y**2)\n",
    "objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Equality(Expression(AFFINE, UNKNOWN, ()), Constant(CONSTANT, NONNEGATIVE, ())),\n",
       " Inequality(Constant(CONSTANT, NONNEGATIVE, ()))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define constraints\n",
    "constraints = [x + y == 1, x - y >= 1]\n",
    "constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000004"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formulate and solve the problem\n",
    "problem = cp.Problem(objective, constraints)\n",
    "problem.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value: 1.0000000000000004\n",
      "Optimal x: 1.0000000000000002\n",
      "Optimal y: 3.9305419627733755e-23\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Optimal value:\", problem.value)\n",
    "print(\"Optimal x:\", x.value)\n",
    "print(\"Optimal y:\", y.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value: 11.999999995678937\n",
      "Optimal x: 3.9999999986829056\n",
      "Optimal y: -1.8488909965547973e-10\n"
     ]
    }
   ],
   "source": [
    "x = cp.Variable()\n",
    "y = cp.Variable()\n",
    "\n",
    "# Objective\n",
    "objective = cp.Maximize(3*x + 2*y)\n",
    "\n",
    "# Constraints\n",
    "constraints = [x + y <= 4, x - 2*y >= -1, x >= 0, y >= 0]\n",
    "\n",
    "# Formulate and solve\n",
    "problem = cp.Problem(objective, constraints)\n",
    "problem.solve()\n",
    "\n",
    "# Display results\n",
    "print(\"Optimal value:\", problem.value)\n",
    "print(\"Optimal x:\", x.value)\n",
    "print(\"Optimal y:\", y.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Vectors and matrices**\n",
    "* `Variables` can be scalars, vectors, or matrices, meaning they are 0, 1, or 2 dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Expression(UNKNOWN, UNKNOWN, (7, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A scalar variable\n",
    "a = cp.Variable()\n",
    "\n",
    "# Vector variable with shape (5, )\n",
    "X = cp.Variable(5)\n",
    "\n",
    "# Matrix variable with shape (5, 1)\n",
    "W = cp.Variable((5, 1))\n",
    "\n",
    "# Matrix variable with shape (7, 5)\n",
    "Q = cp.Variable((7, 5))\n",
    "\n",
    "R = Q @ W\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = X @ W\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Solves a bounded least-squares problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value: 4.124160456449218\n",
      "Optimal var:\n",
      "[ 2.10263212e-02  1.04441002e-01  7.46439739e-02 -2.49595424e-19\n",
      "  1.15712700e-19 -3.08139871e-20 -2.01204900e-19 -1.38076104e-19]\n"
     ]
    }
   ],
   "source": [
    "# We aim to solve the optimization problem:\n",
    "#       minimize || A.x - b ||^2 ; s.t constraint 0 <= x <= 1\n",
    "#           x\n",
    "\n",
    "#       A => R(mXn), x => R(n), b => R(m)\n",
    "\n",
    "# In mathematical terms, we can write this problem in convex optimization form:\n",
    "#       minimize f(x) = (A.x - b)^T (A.x - b)\n",
    "#           x\n",
    "\n",
    "#       subject to: 0 <= x(i) <= 1  for all i\n",
    "\n",
    "# Minimizing this term -> || A.x - b ||^2, makes A.x as close as possible to b\n",
    "\n",
    "\n",
    "m = 16\n",
    "n = 8\n",
    "\n",
    "np.random.seed(42)\n",
    "A = np.random.randn(m, n) * 2.0\n",
    "b = np.random.rand(m) \n",
    "\n",
    "# let's construct the problem.\n",
    "\n",
    "x = cp.Variable(n)\n",
    "\n",
    "# Objective function: minimize ||A.x - b||^2\n",
    "objective = cp.Minimize(cp.sum_squares(A @ x - b))\n",
    "\n",
    "# Constraints\n",
    "constraints = [x >= 0, x <= 1]\n",
    "\n",
    "# Define the problem and solve\n",
    "problem = cp.Problem(objective, constraints)\n",
    "\n",
    "print(\"Optimal value:\", problem.solve())\n",
    "print(\"Optimal var:\")\n",
    "print(x.value)  # A numpy ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value: 4.124160456449236\n",
      "Optimal x: [0.0210263  0.104441   0.07464397 0.         0.         0.\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "## Solve the above problem using scipy.optimize\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "m = 16\n",
    "n = 8\n",
    "\n",
    "np.random.seed(42)\n",
    "A = np.random.randn(m, n) * 2.0\n",
    "b = np.random.rand(m) \n",
    "\n",
    "\n",
    "# Objective function: || A.x - b || ^2\n",
    "def objective(x):\n",
    "    return np.sum((A @ x - b) ** 2)\n",
    "\n",
    "# Bounds for each element of x: 0 <= x <= 1\n",
    "bounds = [(0, 1) for _ in range(n)]\n",
    "\n",
    "# Initial guess for x (can be zeros or any random guess within bounds)\n",
    "x0 = np.zeros(n)\n",
    "\n",
    "# Solve the problem \n",
    "result = minimize(objective, x0, bounds=bounds) \n",
    "\n",
    "# Display the results\n",
    "print(\"Optimal value:\", result.fun)\n",
    "print(\"Optimal x:\", result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Solves a Quadratic Programming Problem (QPP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "DCPError",
     "evalue": "Problem does not follow DCP rules. Specifically:\nThe objective is not DCP. Its following subexpressions are not:\nQuadForm(var784, [[23.95 0.11 ... 1.08 -3.03]\n [0.11 12.28 ... 7.22 9.48]\n ...\n [1.08 7.22 ... 5.33 5.11]\n [-3.03 9.48 ... 5.11 9.84]])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDCPError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Formulate and solve the problem\u001b[39;00m\n\u001b[0;32m     26\u001b[0m problem \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mProblem(objective, constraints)\n\u001b[1;32m---> 27\u001b[0m optimal_value \u001b[38;5;241m=\u001b[39m \u001b[43mproblem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal value:\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimal_value)\n",
      "File \u001b[1;32mc:\\Users\\Bapan Bairagya\\miniconda3\\envs\\myenv\\lib\\site-packages\\cvxpy\\problems\\problem.py:503\u001b[0m, in \u001b[0;36mProblem.solve\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    502\u001b[0m     solve_func \u001b[38;5;241m=\u001b[39m Problem\u001b[38;5;241m.\u001b[39m_solve\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m solve_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bapan Bairagya\\miniconda3\\envs\\myenv\\lib\\site-packages\\cvxpy\\problems\\problem.py:1073\u001b[0m, in \u001b[0;36mProblem._solve\u001b[1;34m(self, solver, warm_start, verbose, gp, qcp, requires_grad, enforce_dpp, ignore_dpp, canon_backend, **kwargs)\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpack(chain\u001b[38;5;241m.\u001b[39mretrieve(soln))\n\u001b[0;32m   1071\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m-> 1073\u001b[0m data, solving_chain, inverse_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_problem_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_dpp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_dpp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanon_backend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;28mprint\u001b[39m(_NUM_SOLVER_STR)\n",
      "File \u001b[1;32mc:\\Users\\Bapan Bairagya\\miniconda3\\envs\\myenv\\lib\\site-packages\\cvxpy\\problems\\problem.py:646\u001b[0m, in \u001b[0;36mProblem.get_problem_data\u001b[1;34m(self, solver, gp, enforce_dpp, ignore_dpp, verbose, canon_backend, solver_opts)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mkey:\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39minvalidate()\n\u001b[1;32m--> 646\u001b[0m     solving_chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_dpp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_dpp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_dpp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_dpp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcanon_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcanon_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver_opts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_opts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m key\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39msolving_chain \u001b[38;5;241m=\u001b[39m solving_chain\n",
      "File \u001b[1;32mc:\\Users\\Bapan Bairagya\\miniconda3\\envs\\myenv\\lib\\site-packages\\cvxpy\\problems\\problem.py:898\u001b[0m, in \u001b[0;36mProblem._construct_chain\u001b[1;34m(self, solver, gp, enforce_dpp, ignore_dpp, canon_backend, solver_opts)\u001b[0m\n\u001b[0;32m    896\u001b[0m candidate_solvers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_candidate_solvers(solver\u001b[38;5;241m=\u001b[39msolver, gp\u001b[38;5;241m=\u001b[39mgp)\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_candidate_solvers(candidate_solvers)\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstruct_solving_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_solvers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m                               \u001b[49m\u001b[43menforce_dpp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_dpp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mignore_dpp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_dpp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcanon_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcanon_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m                               \u001b[49m\u001b[43msolver_opts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mspecified_solver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bapan Bairagya\\miniconda3\\envs\\myenv\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:228\u001b[0m, in \u001b[0;36mconstruct_solving_chain\u001b[1;34m(problem, candidates, gp, enforce_dpp, ignore_dpp, canon_backend, solver_opts, specified_solver)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(problem\u001b[38;5;241m.\u001b[39mvariables()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SolvingChain(reductions\u001b[38;5;241m=\u001b[39m[ConstantSolver()])\n\u001b[1;32m--> 228\u001b[0m reductions \u001b[38;5;241m=\u001b[39m \u001b[43m_reductions_for_problem_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_opts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# Process DPP status of the problem.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m dpp_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdcp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gp \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdgp\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Bapan Bairagya\\miniconda3\\envs\\myenv\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:143\u001b[0m, in \u001b[0;36m_reductions_for_problem_class\u001b[1;34m(problem, candidates, gp, solver_opts)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m problem\u001b[38;5;241m.\u001b[39mis_dqcp():\n\u001b[0;32m    141\u001b[0m         append \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHowever, the problem does follow DQCP rules. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider calling solve() with `qcp=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DCPError(\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProblem does not follow DCP rules. Specifically:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m append)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m gp \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m problem\u001b[38;5;241m.\u001b[39mis_dgp():\n\u001b[0;32m    146\u001b[0m     append \u001b[38;5;241m=\u001b[39m build_non_disciplined_error_msg(problem, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDGP\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mDCPError\u001b[0m: Problem does not follow DCP rules. Specifically:\nThe objective is not DCP. Its following subexpressions are not:\nQuadForm(var784, [[23.95 0.11 ... 1.08 -3.03]\n [0.11 12.28 ... 7.22 9.48]\n ...\n [1.08 7.22 ... 5.33 5.11]\n [-3.03 9.48 ... 5.11 9.84]])"
     ]
    }
   ],
   "source": [
    "# Objective Function:\n",
    "#       Maximize f(a) = 1^T.a - 0.5 * (a^T.Q.a)\n",
    "#           a\n",
    "# Subject to: y^T.a = 0 and a(i) >= 0 for all i\n",
    "\n",
    "m, n = 12, 4\n",
    "\n",
    "# np.random.seed(47)\n",
    "\n",
    "X = np.random.randn(m, n) * 2.0\n",
    "y = np.random.choice([-1, 1], size=m)\n",
    "\n",
    "# Compute Q matrix\n",
    "Q = np.outer(y, y) * (X @ X.T)\n",
    "\n",
    "# Define the variable alpha (a)\n",
    "alpha = cp.Variable(m)\n",
    "\n",
    "# Define the minimization objective (minimizing -f(alpha))\n",
    "objective = cp.Minimize(-cp.sum(alpha) + 0.5 * cp.quad_form(alpha, Q))\n",
    "\n",
    "# Define constraints\n",
    "constraints = [alpha >= 0, y @ alpha == 0]\n",
    "\n",
    "# Formulate and solve the problem\n",
    "problem = cp.Problem(objective, constraints)\n",
    "optimal_value = problem.solve()\n",
    "\n",
    "# Display the results\n",
    "print(\"Optimal value:\", optimal_value)\n",
    "print(\"Optimal alpha:\", alpha.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value: 762112.9913198742\n",
      "Optimal alpha: [48607.42489041 54078.57256721 32314.1711677  15119.24907212\n",
      " 73598.28044637 57274.07560826 34396.04200658 64947.6398479\n",
      " 69093.52881099 19366.32611972 44840.62382719 52100.85528769\n",
      " 41156.98116012 50516.15382827 65948.17581545 38910.87425279]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# let's try to solve the above using scipy.optimize\n",
    "\n",
    "# Problem data\n",
    "m = 16  # Number of samples (example)\n",
    "n = 5   # Number of features (example)\n",
    "\n",
    "# Randomly generated feature matrix and labels for illustration\n",
    "np.random.seed(111)\n",
    "X = np.random.randn(m, n)\n",
    "y = np.random.choice([-1, 1], size=m)\n",
    "\n",
    "# Compute Q matrix\n",
    "Q = np.outer(y, y) * (X @ X.T)\n",
    "\n",
    "# Define the objective function for the dual SVM\n",
    "def objective(alpha):\n",
    "    return -np.sum(alpha) + 0.5 * alpha @ Q @ alpha\n",
    "\n",
    "\n",
    "# 1.Equality Constraint: y^T.alpha = 0\n",
    "# 2.Inequality Constraint (Bounds): alpha >= 0\n",
    "\n",
    "# In SciPy’s `minimize` function, equality constraints and bounds are handled separately. Let’s look at each in turn.\n",
    "\n",
    "# Equality Constraint: y^T.alpha = 0\n",
    "constraints = [\n",
    "    {'type': 'eq', 'fun': lambda alpha: np.dot(y, alpha)}\n",
    "]\n",
    "\n",
    "# Bounds for each alpha: alpha >= 0 \n",
    "bounds = [(0, None) for _ in range(m)]  \n",
    "\n",
    "# Initial guess for alpha\n",
    "alpha0 = np.zeros(m)\n",
    "\n",
    "# Solve the problem\n",
    "result = minimize(objective, alpha0, bounds=bounds, constraints=constraints)\n",
    "\n",
    "# Display the results\n",
    "print(\"Optimal value:\", -result.fun)  # Negate the result because we minimized -f(alpha)\n",
    "print(\"Optimal alpha:\", result.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value: 63.09258011944651\n",
      "Optimal alpha: [5.66146425e+00 0.00000000e+00 0.00000000e+00 1.00000000e+01\n",
      " 1.00000000e+01 9.30800982e-13 2.02821610e-12 2.36553088e+00\n",
      " 2.51334964e+00 0.00000000e+00 4.19071700e+00 1.00000000e+01\n",
      " 1.00000000e+01 1.00000000e+01 0.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Problem data\n",
    "n = 16  # Number of samples (example)\n",
    "d = 4   # Number of features (example)\n",
    "C = 10.0  # Regularization parameter (upper bound for alpha)\n",
    "\n",
    "# Randomly generated feature matrix and labels for illustration\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(n, d)\n",
    "y = np.random.choice([-1, 1], size=n)\n",
    "\n",
    "# Compute the Gram matrix K\n",
    "K = X @ X.T\n",
    "\n",
    "# Objective function: Maximize f(alpha) = 1^T * alpha - 0.5 * (alpha * y)^T * K * (alpha * y)\n",
    "def objective(alpha):\n",
    "    alpha_y = alpha * y  # Element-wise product of alpha and y\n",
    "    return -np.sum(alpha) + 0.5 * np.dot(alpha_y, K @ alpha_y)\n",
    "\n",
    "# Equality constraint: alpha^T * y = 0\n",
    "constraints = [\n",
    "    {'type': 'eq', 'fun': lambda alpha: np.dot(y, alpha)}\n",
    "]\n",
    "\n",
    "# Bounds for each alpha: 0 <= alpha <= C\n",
    "bounds = [(0, C) for _ in range(n)]\n",
    "\n",
    "# Initial guess for alpha\n",
    "alpha0 = np.zeros(n)\n",
    "\n",
    "# Solve the problem using scipy.optimize.minimize\n",
    "result = minimize(objective, alpha0, bounds=bounds, constraints=constraints, method='SLSQP')\n",
    "\n",
    "# Display the results\n",
    "print(\"Optimal value:\", -result.fun)  # Negate the result because we maximized\n",
    "print(\"Optimal alpha:\", result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function minimize in module scipy.optimize._minimize:\n",
      "\n",
      "minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "    Minimization of scalar function of one or more variables.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    fun : callable\n",
      "        The objective function to be minimized.\n",
      "    \n",
      "            ``fun(x, *args) -> float``\n",
      "    \n",
      "        where ``x`` is a 1-D array with shape (n,) and ``args``\n",
      "        is a tuple of the fixed parameters needed to completely\n",
      "        specify the function.\n",
      "    x0 : ndarray, shape (n,)\n",
      "        Initial guess. Array of real elements of size (n,),\n",
      "        where ``n`` is the number of independent variables.\n",
      "    args : tuple, optional\n",
      "        Extra arguments passed to the objective function and its\n",
      "        derivatives (`fun`, `jac` and `hess` functions).\n",
      "    method : str or callable, optional\n",
      "        Type of solver.  Should be one of\n",
      "    \n",
      "            - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "            - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "            - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "            - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "            - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "            - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "            - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "            - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "            - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "            - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
      "            - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "            - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "            - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
      "            - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
      "            - custom - a callable object, see below for description.\n",
      "    \n",
      "        If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "        depending on whether or not the problem has constraints or bounds.\n",
      "    jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n",
      "        Method for computing the gradient vector. Only for CG, BFGS,\n",
      "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
      "        trust-exact and trust-constr.\n",
      "        If it is a callable, it should be a function that returns the gradient\n",
      "        vector:\n",
      "    \n",
      "            ``jac(x, *args) -> array_like, shape (n,)``\n",
      "    \n",
      "        where ``x`` is an array with shape (n,) and ``args`` is a tuple with\n",
      "        the fixed parameters. If `jac` is a Boolean and is True, `fun` is\n",
      "        assumed to return a tuple ``(f, g)`` containing the objective\n",
      "        function and the gradient.\n",
      "        Methods 'Newton-CG', 'trust-ncg', 'dogleg', 'trust-exact', and\n",
      "        'trust-krylov' require that either a callable be supplied, or that\n",
      "        `fun` return the objective and gradient.\n",
      "        If None or False, the gradient will be estimated using 2-point finite\n",
      "        difference estimation with an absolute step size.\n",
      "        Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used\n",
      "        to select a finite difference scheme for numerical estimation of the\n",
      "        gradient with a relative step size. These finite difference schemes\n",
      "        obey any specified `bounds`.\n",
      "    hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy}, optional\n",
      "        Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n",
      "        trust-ncg, trust-krylov, trust-exact and trust-constr.\n",
      "        If it is callable, it should return the Hessian matrix:\n",
      "    \n",
      "            ``hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)``\n",
      "    \n",
      "        where ``x`` is a (n,) ndarray and ``args`` is a tuple with the fixed\n",
      "        parameters.\n",
      "        The keywords {'2-point', '3-point', 'cs'} can also be used to select\n",
      "        a finite difference scheme for numerical estimation of the hessian.\n",
      "        Alternatively, objects implementing the `HessianUpdateStrategy`\n",
      "        interface can be used to approximate the Hessian. Available\n",
      "        quasi-Newton methods implementing this interface are:\n",
      "    \n",
      "            - `BFGS`;\n",
      "            - `SR1`.\n",
      "    \n",
      "        Not all of the options are available for each of the methods; for\n",
      "        availability refer to the notes.\n",
      "    hessp : callable, optional\n",
      "        Hessian of objective function times an arbitrary vector p. Only for\n",
      "        Newton-CG, trust-ncg, trust-krylov, trust-constr.\n",
      "        Only one of `hessp` or `hess` needs to be given. If `hess` is\n",
      "        provided, then `hessp` will be ignored. `hessp` must compute the\n",
      "        Hessian times an arbitrary vector:\n",
      "    \n",
      "            ``hessp(x, p, *args) ->  ndarray shape (n,)``\n",
      "    \n",
      "        where ``x`` is a (n,) ndarray, ``p`` is an arbitrary vector with\n",
      "        dimension (n,) and ``args`` is a tuple with the fixed\n",
      "        parameters.\n",
      "    bounds : sequence or `Bounds`, optional\n",
      "        Bounds on variables for Nelder-Mead, L-BFGS-B, TNC, SLSQP, Powell,\n",
      "        trust-constr, and COBYLA methods. There are two ways to specify the\n",
      "        bounds:\n",
      "    \n",
      "            1. Instance of `Bounds` class.\n",
      "            2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "               is used to specify no bound.\n",
      "    \n",
      "    constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
      "        Constraints definition. Only for COBYLA, SLSQP and trust-constr.\n",
      "    \n",
      "        Constraints for 'trust-constr' are defined as a single object or a\n",
      "        list of objects specifying constraints to the optimization problem.\n",
      "        Available constraints are:\n",
      "    \n",
      "            - `LinearConstraint`\n",
      "            - `NonlinearConstraint`\n",
      "    \n",
      "        Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n",
      "        Each dictionary with fields:\n",
      "    \n",
      "            type : str\n",
      "                Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "            fun : callable\n",
      "                The function defining the constraint.\n",
      "            jac : callable, optional\n",
      "                The Jacobian of `fun` (only for SLSQP).\n",
      "            args : sequence, optional\n",
      "                Extra arguments to be passed to the function and Jacobian.\n",
      "    \n",
      "        Equality constraint means that the constraint function result is to\n",
      "        be zero whereas inequality means that it is to be non-negative.\n",
      "        Note that COBYLA only supports inequality constraints.\n",
      "    tol : float, optional\n",
      "        Tolerance for termination. When `tol` is specified, the selected\n",
      "        minimization algorithm sets some relevant solver-specific tolerance(s)\n",
      "        equal to `tol`. For detailed control, use solver-specific\n",
      "        options.\n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods except `TNC` accept the\n",
      "        following generic options:\n",
      "    \n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform. Depending on the\n",
      "                method each iteration may use several function evaluations.\n",
      "    \n",
      "                For `TNC` use `maxfun` instead of `maxiter`.\n",
      "            disp : bool\n",
      "                Set to True to print convergence messages.\n",
      "    \n",
      "        For method-specific options, see :func:`show_options()`.\n",
      "    callback : callable, optional\n",
      "        A callable called after each iteration.\n",
      "    \n",
      "        All methods except TNC, SLSQP, and COBYLA support a callable with\n",
      "        the signature:\n",
      "    \n",
      "            ``callback(intermediate_result: OptimizeResult)``\n",
      "    \n",
      "        where ``intermediate_result`` is a keyword parameter containing an\n",
      "        `OptimizeResult` with attributes ``x`` and ``fun``, the present values\n",
      "        of the parameter vector and objective function. Note that the name\n",
      "        of the parameter must be ``intermediate_result`` for the callback\n",
      "        to be passed an `OptimizeResult`. These methods will also terminate if\n",
      "        the callback raises ``StopIteration``.\n",
      "    \n",
      "        All methods except trust-constr (also) support a signature like:\n",
      "    \n",
      "            ``callback(xk)``\n",
      "    \n",
      "        where ``xk`` is the current parameter vector.\n",
      "    \n",
      "        Introspection is used to determine which of the signatures above to\n",
      "        invoke.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        The optimization result represented as a ``OptimizeResult`` object.\n",
      "        Important attributes are: ``x`` the solution array, ``success`` a\n",
      "        Boolean flag indicating if the optimizer exited successfully and\n",
      "        ``message`` which describes the cause of the termination. See\n",
      "        `OptimizeResult` for a description of other attributes.\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    minimize_scalar : Interface to minimization algorithms for scalar\n",
      "        univariate functions\n",
      "    show_options : Additional options accepted by the solvers\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter. The default method is *BFGS*.\n",
      "    \n",
      "    **Unconstrained minimization**\n",
      "    \n",
      "    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "    gradient algorithm by Polak and Ribiere, a variant of the\n",
      "    Fletcher-Reeves method described in [5]_ pp.120-122. Only the\n",
      "    first derivatives are used.\n",
      "    \n",
      "    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "    pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "    performance even for non-smooth optimizations. This method also\n",
      "    returns an approximation of the Hessian inverse, stored as\n",
      "    `hess_inv` in the OptimizeResult object.\n",
      "    \n",
      "    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "    Newton method). It uses a CG method to the compute the search\n",
      "    direction. See also *TNC* method for a box-constrained\n",
      "    minimization with a similar algorithm. Suitable for large-scale\n",
      "    problems.\n",
      "    \n",
      "    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "    trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "    algorithm requires the gradient and Hessian; furthermore the\n",
      "    Hessian is required to be positive definite.\n",
      "    \n",
      "    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "    Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "    unconstrained minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
      "    the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
      "    minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    On indefinite problems it requires usually less iterations than the\n",
      "    `trust-ncg` method and is recommended for medium and large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
      "    is a trust-region method for unconstrained minimization in which\n",
      "    quadratic subproblems are solved almost exactly [13]_. This\n",
      "    algorithm requires the gradient and the Hessian (which is\n",
      "    *not* required to be positive definite). It is, in many\n",
      "    situations, the Newton method to converge in fewer iterations\n",
      "    and the most recommended for small and medium-size problems.\n",
      "    \n",
      "    **Bound-Constrained minimization**\n",
      "    \n",
      "    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "    applications. However, if numerical computation of derivative can be\n",
      "    trusted, other algorithms using the first and/or second derivatives\n",
      "    information might be preferred for their better performance in\n",
      "    general.\n",
      "    \n",
      "    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "    algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "    \n",
      "    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "    of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "    method. It performs sequential one-dimensional minimizations along\n",
      "    each vector of the directions set (`direc` field in `options` and\n",
      "    `info`), which is updated at each iteration of the main\n",
      "    minimization loop. The function need not be differentiable, and no\n",
      "    derivatives are taken. If bounds are not provided, then an\n",
      "    unbounded line search will be used. If bounds are provided and\n",
      "    the initial guess is within the bounds, then every function\n",
      "    evaluation throughout the minimization procedure will be within\n",
      "    the bounds. If bounds are provided, the initial guess is outside\n",
      "    the bounds, and `direc` is full rank (default has full rank), then\n",
      "    some function evaluations during the first iteration may be\n",
      "    outside the bounds, but every function evaluation after the first\n",
      "    iteration will be within the bounds. If `direc` is not full rank,\n",
      "    then some parameters may not be optimized and the solution is not\n",
      "    guaranteed to be within the bounds.\n",
      "    \n",
      "    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "    algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "    to bounds. This algorithm uses gradient information; it is also\n",
      "    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "    method described above as it wraps a C implementation and allows\n",
      "    each variable to be given upper and lower bounds.\n",
      "    \n",
      "    **Constrained Minimization**\n",
      "    \n",
      "    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "    Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "    [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "    approximations to the objective function and each constraint. The\n",
      "    method wraps a FORTRAN implementation of the algorithm. The\n",
      "    constraints functions 'fun' may return either a single number\n",
      "    or an array or list of numbers.\n",
      "    \n",
      "    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "    Least SQuares Programming to minimize a function of several\n",
      "    variables with any combination of bounds, equality and inequality\n",
      "    constraints. The method wraps the SLSQP Optimization subroutine\n",
      "    originally implemented by Dieter Kraft [12]_. Note that the\n",
      "    wrapper handles infinite values in bounds by converting them into\n",
      "    large floating values.\n",
      "    \n",
      "    Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n",
      "    trust-region algorithm for constrained optimization. It switches\n",
      "    between two implementations depending on the problem definition.\n",
      "    It is the most versatile constrained minimization algorithm\n",
      "    implemented in SciPy and the most appropriate for large-scale problems.\n",
      "    For equality constrained problems it is an implementation of Byrd-Omojokun\n",
      "    Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n",
      "    inequality constraints are imposed as well, it switches to the trust-region\n",
      "    interior point method described in [16]_. This interior point algorithm,\n",
      "    in turn, solves inequality constraints by introducing slack variables\n",
      "    and solving a sequence of equality-constrained barrier problems\n",
      "    for progressively smaller values of the barrier parameter.\n",
      "    The previously described equality constrained SQP method is\n",
      "    used to solve the subproblems with increasing levels of accuracy\n",
      "    as the iterate gets closer to a solution.\n",
      "    \n",
      "    **Finite-Difference Options**\n",
      "    \n",
      "    For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n",
      "    the gradient and the Hessian may be approximated using\n",
      "    three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n",
      "    The scheme 'cs' is, potentially, the most accurate but it\n",
      "    requires the function to correctly handle complex inputs and to\n",
      "    be differentiable in the complex plane. The scheme '3-point' is more\n",
      "    accurate than '2-point' but requires twice as many operations. If the\n",
      "    gradient is estimated via finite-differences the Hessian must be\n",
      "    estimated using one of the quasi-Newton strategies.\n",
      "    \n",
      "    **Method specific options for the** `hess` **keyword**\n",
      "    \n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | method/Hess  | None | callable | '2-point/'3-point'/'cs' | HUS |\n",
      "    +==============+======+==========+=========================+=====+\n",
      "    | Newton-CG    | x    | (n, n)   | x                       | x   |\n",
      "    |              |      | LO       |                         |     |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | dogleg       |      | (n, n)   |                         |     |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | trust-ncg    |      | (n, n)   | x                       | x   |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | trust-krylov |      | (n, n)   | x                       | x   |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | trust-exact  |      | (n, n)   |                         |     |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    | trust-constr | x    | (n, n)   |  x                      | x   |\n",
      "    |              |      | LO       |                         |     |\n",
      "    |              |      | sp       |                         |     |\n",
      "    +--------------+------+----------+-------------------------+-----+\n",
      "    \n",
      "    where LO=LinearOperator, sp=Sparse matrix, HUS=HessianUpdateStrategy\n",
      "    \n",
      "    **Custom minimizers**\n",
      "    \n",
      "    It may be useful to pass a custom minimization method, for example\n",
      "    when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "    or a different library.  You can simply pass a callable as the ``method``\n",
      "    parameter.\n",
      "    \n",
      "    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "    its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "    `fun` returns just the function values and `jac` is converted to a function\n",
      "    returning the Jacobian.  The method shall return an `OptimizeResult`\n",
      "    object.\n",
      "    \n",
      "    The provided `method` callable must be able to accept (and possibly ignore)\n",
      "    arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "    expand in future versions and then these parameters will be passed to\n",
      "    the method.  You can find an example in the scipy.optimize tutorial.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "        Minimization. The Computer Journal 7: 308-13.\n",
      "    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "        191-208.\n",
      "    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "       a function of several variables without calculating derivatives. The\n",
      "       Computer Journal 7: 155-162.\n",
      "    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "       Numerical Recipes (any edition), Cambridge University Press.\n",
      "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "       Springer New York.\n",
      "    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "       Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "       Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "       optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "       550-560.\n",
      "    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "    .. [9] Powell, M J D. A direct search optimization method that models\n",
      "       the objective and constraint functions by linear interpolation.\n",
      "       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "    .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "       calculations. 1998. Acta Numerica 7: 287-336.\n",
      "    .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "       derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "       2007/NA03\n",
      "    .. [12] Kraft, D. A software package for sequential quadratic\n",
      "       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "       Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "    .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
      "       Trust region methods. 2000. Siam. pp. 169-200.\n",
      "    .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
      "       implementation of the GLTR method for iterative solution of\n",
      "       the trust region problem\", :arxiv:`1611.04718`\n",
      "    .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
      "       Trust-Region Subproblem using the Lanczos Method\",\n",
      "       SIAM J. Optim., 9(2), 504--525, (1999).\n",
      "    .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n",
      "        An interior point algorithm for large-scale nonlinear  programming.\n",
      "        SIAM Journal on Optimization 9.4: 877-900.\n",
      "    .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the\n",
      "        implementation of an algorithm for large-scale equality constrained\n",
      "        optimization. SIAM Journal on Optimization 8.3: 682-706.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "    function (and its respective derivatives) is implemented in `rosen`\n",
      "    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "    \n",
      "    >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "    \n",
      "    A simple application of the *Nelder-Mead* method is:\n",
      "    \n",
      "    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    \n",
      "    Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "    options:\n",
      "    \n",
      "    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "    ...                options={'gtol': 1e-6, 'disp': True})\n",
      "    Optimization terminated successfully.\n",
      "             Current function value: 0.000000\n",
      "             Iterations: 26\n",
      "             Function evaluations: 31\n",
      "             Gradient evaluations: 31\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    >>> print(res.message)\n",
      "    Optimization terminated successfully.\n",
      "    >>> res.hess_inv\n",
      "    array([\n",
      "        [ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "        [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "        [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "        [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "        [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]\n",
      "    ])\n",
      "    \n",
      "    \n",
      "    Next, consider a minimization problem with several constraints (namely\n",
      "    Example 16.4 from [5]_). The objective function is:\n",
      "    \n",
      "    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "    \n",
      "    There are three constraints defined as:\n",
      "    \n",
      "    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "    \n",
      "    And variables must be positive, hence the following bounds:\n",
      "    \n",
      "    >>> bnds = ((0, None), (0, None))\n",
      "    \n",
      "    The optimization problem is solved using the SLSQP method as:\n",
      "    \n",
      "    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "    ...                constraints=cons)\n",
      "    \n",
      "    It should converge to the theoretical solution (1.4 ,1.7).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(scipy.optimize.minimize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
